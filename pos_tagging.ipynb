{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "88af196f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal, List, Tuple, cast, Dict\n",
    "from collections import Counter\n",
    "from transformers import (\n",
    "    BertTokenizer,\n",
    "    AutoModelForTokenClassification,\n",
    "    BatchEncoding,\n",
    "    TrainingArguments,\n",
    "    DataCollatorForTokenClassification,\n",
    "    Trainer,\n",
    ")\n",
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "63424622",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0bdd0cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "PrimitiveDataset = List[Tuple[List[str], List[str]]]\n",
    "\n",
    "NUM_TAGS = 16\n",
    "\n",
    "TAG2IDX = {\n",
    "    \"ADP\": 0,\n",
    "    \"NOUN\": 1,\n",
    "    \"PUNCT\": 2,\n",
    "    \"VERB\": 3,\n",
    "    \"AUX\": 4,\n",
    "    \"PRON\": 5,\n",
    "    \"ADJ\": 6,\n",
    "    \"PART\": 7,\n",
    "    \"ADV\": 8,\n",
    "    \"INTJ\": 9,\n",
    "    \"DET\": 10,\n",
    "    \"PROPN\": 11,\n",
    "    \"CCONJ\": 12,\n",
    "    \"NUM\": 13,\n",
    "    \"SCONJ\": 14,\n",
    "    \"X\": 15,\n",
    "}\n",
    "IDX2TAG = {\n",
    "    0: \"ADP\",\n",
    "    1: \"NOUN\",\n",
    "    2: \"PUNCT\",\n",
    "    3: \"VERB\",\n",
    "    4: \"AUX\",\n",
    "    5: \"PRON\",\n",
    "    6: \"ADJ\",\n",
    "    7: \"PART\",\n",
    "    8: \"ADV\",\n",
    "    9: \"INTJ\",\n",
    "    10: \"DET\",\n",
    "    11: \"PROPN\",\n",
    "    12: \"CCONJ\",\n",
    "    13: \"NUM\",\n",
    "    14: \"SCONJ\",\n",
    "    15: \"X\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c0ca731d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_dataset(\n",
    "    dataset: Literal[\"train\"] | Literal[\"dev\"] | Literal[\"test\"],\n",
    ") -> PrimitiveDataset:\n",
    "    assert dataset in [\"train\", \"dev\", \"test\"]\n",
    "\n",
    "    tokens = []\n",
    "\n",
    "    with open(f\"./corpus/bg_btb-ud-{dataset}.conllu\") as file:\n",
    "        sents = file.read().split(\"\\n\" * 2)\n",
    "        for sent in sents:\n",
    "            if not sent:\n",
    "                continue\n",
    "\n",
    "            sent_words = []\n",
    "            sent_pos_types = []\n",
    "\n",
    "            rows = sent.split(\"\\n\")\n",
    "            for r in rows:\n",
    "                if r[0] == \"#\":\n",
    "                    continue\n",
    "                _, word, _, pos_type, *_ = r.split(\"\\t\")\n",
    "                sent_words.append(word)\n",
    "                sent_pos_types.append(pos_type)\n",
    "\n",
    "            tokens.append((sent_words, sent_pos_types))\n",
    "\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e3ab6a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_tokens(dataset: PrimitiveDataset) -> Counter:\n",
    "    tokens = [token for (_, sent_tokens) in dataset for token in sent_tokens]\n",
    "    return Counter(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "387653b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_align_labels(examples, tokenizer):\n",
    "    tokenized_inputs = tokenizer(\n",
    "        examples[\"tokens\"], truncation=True, is_split_into_words=True\n",
    "    )\n",
    "\n",
    "    labels = []\n",
    "    for i, label in enumerate(examples[\"ner_tags\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(\n",
    "            batch_index=i\n",
    "        )  # Map tokens to their respective word.\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:  # Set the special tokens to -100.\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            elif (\n",
    "                word_idx != previous_word_idx\n",
    "            ):  # Only label the first token of a given word.\n",
    "                label_ids.append(label[word_idx])\n",
    "            else:\n",
    "                label_ids.append(-100)\n",
    "            previous_word_idx = word_idx\n",
    "        labels.append(label_ids)\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ee2895b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class POSDataset(Dataset):\n",
    "    def __init__(self, pds: PrimitiveDataset, tokenizer):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.pds = pds\n",
    "        self.tds = []\n",
    "        for sent_words, sent_tokens in self.pds:\n",
    "            pbe = tokenize_and_align(sent_words, sent_tokens, self.tokenizer)\n",
    "            self.tds.append(\n",
    "                {\n",
    "                    \"input_ids\": torch.tensor(pbe[\"input_ids\"]),\n",
    "                    \"attention_mask\": torch.tensor(pbe[\"attention_mask\"]),\n",
    "                    \"labels\": torch.tensor(pbe[\"labels\"]),\n",
    "                }\n",
    "            )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.tds)\n",
    "\n",
    "    def __getitem__(self, idx):  # type: ignore (the LSP complains)\n",
    "        return self.tds[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d73a50df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_prediction):\n",
    "    (predictions, label_ids) = eval_prediction\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    compare_tuples = [t for t in zip(predictions, label_ids) if t[0] != -100]\n",
    "    total = len(compare_tuples)\n",
    "    correct = sum(1 for t in compare_tuples if t[0] == t[1])\n",
    "\n",
    "    return {\"accuracy\": correct / total}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "93395d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_trainer(model, train_dataset, test_dataset, tokenzier):\n",
    "    # https://huggingface.co/docs/transformers/main_classes/trainer#transformers.TrainingArguments\n",
    "    training_args = TrainingArguments(\n",
    "        num_train_epochs=5,\n",
    "        per_device_train_batch_size=16,\n",
    "        per_device_eval_batch_size=32,\n",
    "        learning_rate=2e-5,\n",
    "        weight_decay=0.01,\n",
    "        eval_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        load_best_model_at_end=True,\n",
    "        push_to_hub=False,\n",
    "        warmup_ratio=0.1,\n",
    "        fp16=True,\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=test_dataset,\n",
    "        processing_class=tokenizer,\n",
    "        data_collator=DataCollatorForTokenClassification(tokenizer),\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "\n",
    "    return trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5a3608a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:104: UserWarning: \n",
      "Error while fetching `HF_TOKEN` secret value from your vault: 'Requesting secret HF_TOKEN timed out. Secrets can only be fetched when running from the Colab UI.'.\n",
      "You are not authenticated with the Hugging Face Hub in this notebook.\n",
      "If the error persists, please let us know by opening an issue on GitHub (https://github.com/huggingface/huggingface_hub/issues/new).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8362500381934f5f9c84c882652af874",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/49.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c280423e30f493fad7456d3bf093d2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/996k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5686b49025b245dba378a4ca7f4c1e0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.96M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './corpus/bg_btb-ud-train.conllu'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-441136427.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBertTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"bert-base-multilingual-cased\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBertTokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtrain_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPOSDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparse_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"train\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mtest_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPOSDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparse_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"test\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# print(dataset[0])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipython-input-690167834.py\u001b[0m in \u001b[0;36mparse_dataset\u001b[0;34m(dataset)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"./corpus/bg_btb-ud-{dataset}.conllu\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0msents\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n\"\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msents\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './corpus/bg_btb-ud-train.conllu'"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-multilingual-cased\")\n",
    "tokenizer = cast(BertTokenizer, tokenizer)\n",
    "train_dataset = POSDataset(parse_dataset(\"train\"), tokenizer)\n",
    "test_dataset = POSDataset(parse_dataset(\"test\"), tokenizer)\n",
    "# print(dataset[0])\n",
    "# print(dataset[0][\"labels\"].size())\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    \"bert-base-multilingual-cased\",\n",
    "    num_labels=NUM_TAGS,\n",
    "    id2label=IDX2TAG,\n",
    "    label2id=TAG2IDX,\n",
    ")\n",
    "print(f\"Total parameters: {sum(p.numel() for p in model.parameters())}\")\n",
    "trainer = create_trainer(model, train_dataset, test_dataset, tokenizer)\n",
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
